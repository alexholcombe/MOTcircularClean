#This file analyses anonymized data provided by "loadAnonymiseSaveData.R" in exp-specific directory
#This script expects the working directory to be "analysis"
rm(list = ls()) #Clear workspace
library(tidyverse)

dataDir<- file.path("..","dataAnonymized")
expName<- "Momo"

#Read all the Psychopy data in, from the .tsv file generated by loadAnonymiseSaveData.R in dataPreprocess directory
anonDataFile<- paste0(expName,".tsv") 
anonDataFileWithPath<-file.path(dataDir,expName,anonDataFile)
if (!file.exists(anonDataFileWithPath)) {
  message("Your anonymised datafile doesn't exist.")
}
rawData<- readr::read_tsv(anonDataFileWithPath,  show_col_types=FALSE)

#Exclusions
excludeFixationViolations = TRUE
proportnTrialsMustFixate = .6
excludeDidntReach75pct = TRUE
maxTimingBlipsToIncludeTrial = 6
#Eyemovement exclusions
exclusionDeg = 2.38 #in any direction from fixation. Momo told me to use this, that corresponds to 50 pixels
widthPix = 800
heightPix = 600
monitorWidth = 39.5 #cm
viewdist = 57 #cm
widthScreenDeg =  2*(atan((monitorWidth/2)/viewdist) /pi*180)
pixelsPerDegree = widthPix / widthScreenDeg
exclusionPixels = exclusionDeg * pixelsPerDegree
centralZoneWidthPix = exclusionPixels*2
centralZoneHeightPix = exclusionPixels*2 #assumes the monitor is correct aspect ratio so that pixels are square
fixatnDur = 0.8
trackingExtraTime= 1.2 #giving the person time to attend to the cue (secs).
initialDurToExclude<- fixatnDur + trackingExtraTime

#check counterbalancing of this exp
source( file.path("helpers","checkCounterbalancing.R") )
t<- checkCombosOccurEqually(rawData, c("numObjects","offset"), tolerance=.01 )
#Visually inspect trials for each combination for each subject
#table(rawData$numObjects,rawData$offset,rawData$IDnum)

# checkCombosOccurEqually(rawData, c("numObjects","numTargets","ringToQuery") )
# checkCombosOccurEqually(rawData, c("condition","leftOrRight") )
# checkCombosOccurEqually(rawData, c("condition","leftOrRight","offsetXYring0") ) #NO?
# checkCombosOccurEqually(rawData, c("numObjects","numTargets","speed") )  

datAnalyze <-rawData

datAnalyze$correct <- (datAnalyze$orderCorrect==3)
datAnalyze$correct<- as.numeric(datAnalyze$correct)
datAnalyze$chance <- 1 / datAnalyze$numObjects

#Make simpler names of variables for graphs for nicer plot labels, and make them factors
# for better plotting.
datAnalyze<- datAnalyze %>% dplyr::rename(objects = numObjects) %>% mutate(objects = as.factor(objects))
datAnalyze<- datAnalyze |> mutate(offset = as.factor(offset))

datAnalyze$subject<- datAnalyze$IDnum
factorsForBreakdownForAnalysis <- c('objects','offset')

#If staircases work, average correct should be 0.794 in each condition
avgCorrOverall<- datAnalyze |> group_by(subject) |> summarise(mean=mean(correct),n=n())
avgCorr<- ggplot(avgCorrOverall,aes(x=subject,y=mean)) + geom_point()
#show(avgCorr)

######################################################################################
#EYE MOVEMENTS
#Exclude trials where participant didn't fixate at center
##

#Load table of EDF file name correspondences, from the .tsv file generated by loadAnonymiseSaveData.R in dataPreprocess directory
EDFmatchFile<- paste0(expName,"_files_guide.tsv") 
withPath<-file.path(dataDir,expName,EDFmatchFile)
if (!file.exists(withPath)) {
  message("Your EDFmatchFile doesn't exist.")
}
EDFmatchTable<- readr::read_tsv(withPath,  show_col_types=FALSE)

datWithEyeTracking <- datAnalyze |> filter(EDFmatchExists == TRUE)

#Delete session (which might have letters instead of numbers) because redundant with sessionNum 
datWithEyeTracking$session<-NULL; EDFmatchTable$session<- NULL

#Join with EDFmatchTable so know the EDF file for each participant
datWithEyeTracking<- datWithEyeTracking |> 
  left_join(EDFmatchTable, 
            by=join_by(IDnum,sessionNum, #colnames in next lines are redundant ones to avoid doubling them
                       comment,pTrialsLotsTimingBlips,pTrialsBlipsAfterFixatn,
                       pTrialsLongFramesAfterCue,EDFmatchExists))

#Manually set some bad files' EDFmatchExists to FALSE so don't get error analysing them
#j182.EDF is a corrupt EDF file, and Eyelink suggested diagnostics that Momo hasn't done
datWithEyeTracking<- datWithEyeTracking %>% 
  mutate(EDFmatchExists = ifelse(str_starts(EDF_name,"j182"),
                                 FALSE, EDFmatchExists) )
#j022 and p014 "wonâ€™t have any data recorded in the file from the problem i was having at the beginning of the experiment"-Momo
datWithEyeTracking<- datWithEyeTracking %>% 
  mutate(EDFmatchExists = ifelse(str_starts(EDF_name,"j022"),
                                 FALSE, EDFmatchExists) )
datWithEyeTracking<- datWithEyeTracking %>% 
  mutate(EDFmatchExists = ifelse(str_starts(EDF_name,"p014"),
                                 FALSE, EDFmatchExists) )

#For each participant, analyze their EDF file and exclude data on that basis
#But first need to handle those where EDFmatchExists==FALSE. Throw them out.
datWithEyeTracking <- datWithEyeTracking |> filter(EDFmatchExists == TRUE)

datWithEyeTracking$EDF_name<- str_c(datWithEyeTracking$EDF_name,".EDF")
#Add path for EDF file
datWithEyeTracking$EDF_name<- file.path(dataDir,expName,"AllEdfData",datWithEyeTracking$EDF_name)
                         
#Function that runs EDFsummarise on each EDF file
# then for each EDF file, we have whether to exclude every trial
# After all that, join with datAnalyze

source( file.path('..','dataPreprocess','eyetracking','summariseFromEDF.R') ) #defines EDFsummarise function

driftCorrect<- FALSE #Currently not implemented
intervalToAssumeGazeCentral<-c(0,0) #Currently not implemented

#The problem with doing it this way is that reading and analysing the EDF file takes a long time,
#   and dies if there is a problem with an EDF file, so it's probably better to do it incrementally
#   with a for loop!
#datWithEyeMetrics<- datWithEyeTracking |> rowwise() |>
#  reframe( EDFsummarise(pick(EDF_name),widthPix,heightPix,centralZoneWidthPix,centralZoneHeightPix,
#                        initialDurToExclude,driftCorrect,intervalToAssumeGazeCentral)         ) 

#Rather than doing it the fancy way above, do it separately so can save the result of analyzing all the 
#eyetracking files separately.
#Run EDF summarise on each row of the list of EDF names, https://gist.github.com/alexholcombe/2ec141beec36ac7640fa296dd236203c
EDFsToAnalyze<-  datWithEyeTracking |> 
                      group_by(IDnum,sessionNum) |>
                      summarize(EDF_name = first(EDF_name), .groups = 'drop')
pathsOnly<- EDFsToAnalyze$EDF_name
#pathsOnly<-pathsOnly[1:3] #Shorten for test run

# Call the function with each element of the list as the first parameter and concatenate the results into a tibble, https://gist.github.com/alexholcombe/2ec141beec36ac7640fa296dd236203c
EDFresults <- map_dfr(pathsOnly, ~ EDFsummarise(.x, widthPix,heightPix,centralZoneWidthPix,centralZoneHeightPix,
                                                    initialDurToExclude,driftCorrect,intervalToAssumeGazeCentral)   )
#It returns a weirdly-structured list actually, because some things only one value per file, others different value each trial,
#   which is in the perTrialStuff
eachTrialEDFs<- EDFresults$perTrialStuff
#Now add in the things that were not calculated for each trial, but I do want them as columns
#Isolate them by getting rid of the perTrialStuff
EDFresults$perTrialStuff <- NULL
#Now can combine them
EDFresults<- cbind(eachTrialEDFs,EDFresults)
  
#Add in the other columns (IDnum, sessionNum) from EDFsToAnalyze
EDFresults<- EDFsToAnalyze |> right_join(EDFresults, by=join_by(EDF_name) )
#table(EDFresults$outOfCentralArea,useNA="ifany")

#Now I have the following columns thanks to EDFsummarise:
# IDnum sessionNum  trial longestDist outOfCentralArea totalBlinkDur blinksTooLong pSamplesFailed anyDataLost

#Join these EDF results with the behavioral data, so can then start excluding trials.
datWithEyeMetrics<- datWithEyeTracking |> 
                      left_join(EDFresults, by=join_by(EDF_name,IDnum,sessionNum, trialnum==trial))


#RUN WITH TIMINGBLIPS ONLY EXCLUDED AND WITH BOTH TIMINGBLIPS AND EYEMOVMENET EXCLUSIONS

#Report on timingBlips
blipsSummary<- datWithEyeMetrics |> 
                  group_by(IDnum) |> 
                  summarise(pTrialsTooManyBlips =  mean(timingBlips > maxTimingBlipsToIncludeTrial) )
write_tsv(blipsSummary, file.path(dataDir,expName,"blipsSummary.tsv"))

#Report on outOfCentralArea
#Work out how much data is missing - because eyetracker didn't work? But I already filtered by 
outOfCentralAreaNA<- datWithEyeMetrics |> 
        group_by(IDnum,sessionNum) |> 
        summarise(NAs = is.na( first(outOfCentralArea) ) )

#summarize(EDF_name = first(EDF_name), .groups = 'drop')


outOfCentralAreaSummary<- datWithEyeMetrics |> 
    group_by(IDnum) |> 
    summarise( pOutOfCentralArea =  mean(outOfCentralArea > 0) )
write_tsv(outOfCentralAreaSummary, file.path(dataDir,expName,"outOfCentralAreaSummary.tsv")
          
                    
#Calculate number of trials per participant with timingBlips and with outOfCentralArea, save as .tsv for Momo

#Then get summariseFromEDF to tell me various metrics about the eyemovements on each trial
#Merge that with datAnalyze
#Perform exclusions
#fixatnPeriod varies 0.8 to 1.3 

  

  proportnTrialsOutside = as.numeric( (eachTrial$outOfCentralArea > 0) )
  msg = paste("Proportion of trials for this participant with any sample outside the central zone =", mean(proportnTrialsOutside))
  message(msg)
  
#https://github.com/alexholcombe/speed-tf-VSS14/blob/master/analyseExps/doAllAnalyses_E4ab.R
iv<-"speed"
for (iv in c("speed","tf")) { #"logTf","logSpd"
  cat('Fitting data, extracting threshes, plotting with iv=',iv)
  
  source('analyzeMakeReadyForPlot.R') #returns fitParms for each subject, psychometrics, and function calcPctCorrThisSpeed
  fitParms$iv<- iv
  #Get the plotIndividDataAndCurves function from another file
  source('individDataWithPsychometricCurves.R') 
  factorsForPlot <- tibble( colorF = "offset", colF = "objects", rowF = "subject" )
  
  #Make a few plots of psychometric functions prior to extracting threshes 
  datForThisPlot <- datAnalyze |> filter(  as.numeric(as.character(subject)) >= 27 ) |>
        filter(  as.numeric(as.character(subject)) <= 999 )
  psychometricsForThisPlot <- psychometrics |> filter( as.numeric(as.character(subject)) >=27  ) |>
    filter(  as.numeric(as.character(subject)) <= 999 )
  
  #Plot all psychometric functions
  plt<- plotIndividDataAndCurves(expName,datForThisPlot,psychometricsForThisPlot,
                           factorsForPlot,wrapOrGrid=T,xmin=0,xmax=1.5) 
  plt<-plt+facet_wrap(vars(subject))
  show(plt)
  ggsave( file.path('figs', paste0('individPlotsE',expName,'.png'))  )
  
  #Psychometric doesn't go high enough with subject= 69  objects= 8  targets= 2, which
  #from the below individual graph looks appropriate - the persom was getting 25% wrong even at low speeds
  #sub69<- datAnalyze |> filter( subject==69)
  #s69<- plotIndividDataAndCurves("subject69",sub69,
  #                         psychometricsForThisPlot |> filter(subject==69),
  #                         tibble( colorF = "targets", colF = "targets", rowF = "objects" ),
  #                         wrapOrGrid=T,xmin=0,xmax=1.5)
  
  #show(s69)
  
  thrAll<-tibble()
  source("extractMomoThreshesAndPlot.R") #provides threshes
  #Add threshes to the plots so that can see where threshold extraction failed
  write_tsv(threshes,file.path("results","threshesMomo.tsv"))
  #below is old way, saving separate threshes
  #   varName=paste("threshes_",iv,"_",expName,sep='') #combine threshes
  #   assign(varName,threshes)
  #   save(list=varName,file=paste(dataDir,varName,".Rdata",sep='')) #e.g. threshes_tf_123targets269objects.Rdata
  #   cat("Saved",varName)
}
