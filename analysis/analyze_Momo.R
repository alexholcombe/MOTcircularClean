#This file analyses anonymized data provided by "loadAnonymiseSaveData.R" in exp-specific directory
#This script expects the working directory to be "analysis"
rm(list = ls()) #Clear workspace
library(tidyverse)

dataDir<- file.path("..","dataAnonymized")
expName<- "Momo"

#Read all the Psychopy data in, from the .tsv file generated by loadAnonymiseSaveData.R in dataPreprocess directory
anonDataFile<- paste0(expName,".tsv") 
anonDataFileWithPath<-file.path(dataDir,expName,anonDataFile)
if (!file.exists(anonDataFileWithPath)) {
  message("Your anonymised datafile doesn't exist.")
}
rawData<- readr::read_tsv(anonDataFileWithPath,  show_col_types=FALSE)

#Exclusions
excludeFixationViolations = TRUE
proportnTrialsMustFixate = .6
excludeDidntReach75pct = TRUE
maxTimingBlipsToIncludeTrial = 6
#Eyemovement exclusions
exclusionDeg = 2.38 #in any direction from fixation. Momo told me to use this, that corresponds to 50 pixels
widthPix = 800
heightPix = 600
monitorWidth = 39.5 #cm
viewdist = 57 #cm
widthScreenDeg =  2*(atan((monitorWidth/2)/viewdist) /pi*180)
pixelsPerDegree = widthPix / widthScreenDeg
exclusionPixels = exclusionDeg * pixelsPerDegree
centralZoneWidthPix = exclusionPixels*2
centralZoneHeightPix = exclusionPixels*2 #assumes the monitor is correct aspect ratio so that pixels are square
fixatnDur = 0.8
trackingExtraTime= 1.2 #giving the person time to attend to the cue (secs).
initialDurToExclude<- fixatnDur + trackingExtraTime

#check counterbalancing of this exp
source( file.path("helpers","checkCounterbalancing.R") )
t<- checkCombosOccurEqually(rawData, c("numObjects","offset"), tolerance=.01 )
#Visually inspect trials for each combination for each subject
#table(rawData$numObjects,rawData$offset,rawData$IDnum)

# checkCombosOccurEqually(rawData, c("numObjects","numTargets","ringToQuery") )
# checkCombosOccurEqually(rawData, c("condition","leftOrRight") )
# checkCombosOccurEqually(rawData, c("condition","leftOrRight","offsetXYring0") ) #NO?
# checkCombosOccurEqually(rawData, c("numObjects","numTargets","speed") )  

datAnalyze <-rawData

datAnalyze$correct <- (datAnalyze$orderCorrect==3)
datAnalyze$correct<- as.numeric(datAnalyze$correct)
datAnalyze$chance <- 1 / datAnalyze$numObjects

#Make simpler names of variables for graphs for nicer plot labels, and make them factors
# for better plotting.
datAnalyze<- datAnalyze %>% dplyr::rename(objects = numObjects) %>% mutate(objects = as.factor(objects))
datAnalyze<- datAnalyze |> mutate(offset = as.factor(offset))

datAnalyze$subject<- datAnalyze$IDnum
factorsForBreakdownForAnalysis <- c('objects','offset')

#If staircases work, average correct should be 0.794 in each condition
avgCorrOverall<- datAnalyze |> group_by(subject) |> summarise(mean=mean(correct),n=n())
avgCorr<- ggplot(avgCorrOverall,aes(x=subject,y=mean)) + geom_point()
#show(avgCorr)

######################################################################################
#EYE MOVEMENTS
#Exclude trials where participant didn't fixate at center
##

#Load table of EDF file name correspondences, from the .tsv file generated by loadAnonymiseSaveData.R in dataPreprocess directory
EDFmatchFile<- paste0(expName,"_files_guide.tsv") 
withPath<-file.path(dataDir,expName,EDFmatchFile)
if (!file.exists(withPath)) {
  message("Your EDFmatchFile doesn't exist.")
}
EDFmatchTable<- readr::read_tsv(withPath,  show_col_types=FALSE)

#Delete P01 because something is screwy with subject ID 01, who somehow has 4 session 2's acccording to my derived records, whereas all others don’t have such duplicates
#       Momo: sorry i totally forgot but P01 I had a problem with running session 2 where I ran it for 1 trials worth on the first run so ran another session with 3 trials worth to make it up to normal (4 trials per session). Also this was when there was a problem with running sessions titled ‘2’ so i believe the 1 trial session would have been P013 and the 3 trial session would have been P014.
datWithEyeTracking<- datAnalyze |> filter(datAnalyze$IDnum != "01")

#Delete session (which might have letters instead of numbers) because redundant with sessionNum 
datWithEyeTracking$session<-NULL; EDFmatchTable$session<- NULL

#Join with EDFmatchTable so know the EDF file for each participant
datWithEyeTracking<- datWithEyeTracking |> 
  left_join(EDFmatchTable, 
            by=join_by(IDnum,sessionNum, #colnames in next lines are redundant ones to avoid doubling them
                       comment,pTrialsLotsTimingBlips,pTrialsBlipsAfterFixatn,
                       pTrialsLongFramesAfterCue,EDFmatchExists))

#Manually set some bad files' EDFmatchExists to FALSE so don't get error analysing them
#j182.EDF is a corrupt EDF file, and Eyelink suggested diagnostics that Momo hasn't done
datWithEyeTracking<- datWithEyeTracking %>% 
  mutate(EDFmatchExists = ifelse(str_starts(EDF_name,"j182"),
                                 FALSE, EDFmatchExists) )
#j022 and p014 "won’t have any data recorded in the file from the problem i was having at the beginning of the experiment"-Momo
datWithEyeTracking<- datWithEyeTracking %>% 
  mutate(EDFmatchExists = ifelse(str_starts(EDF_name,"j022"),
                                 FALSE, EDFmatchExists) )
datWithEyeTracking<- datWithEyeTracking %>% 
  mutate(EDFmatchExists = ifelse(str_starts(EDF_name,"p014"),
                                 FALSE, EDFmatchExists) )

#For each participant, analyze their EDF file and exclude data on that basis
#But first need to handle those where EDFmatchExists==FALSE. Throw them out.
datWithEyeTracking <- datWithEyeTracking |> filter(EDFmatchExists == TRUE)

datWithEyeTracking$EDF_name<- str_c(datWithEyeTracking$EDF_name,".EDF")
#Add path for EDF file
datWithEyeTracking$EDF_name<- file.path(dataDir,expName,"AllEdfData",datWithEyeTracking$EDF_name)
                         
#Get function that runs EDFsummarise on each EDF file
# then for each EDF file, we have whether to exclude every trial
# After all that, join with datAnalyze
source( file.path('..','dataPreprocess','eyetracking','summariseFromEDF.R') ) #defines EDFsummarise function

driftCorrect<- FALSE #Currently not implemented
intervalToAssumeGazeCentral<-c(0,0) #Currently not implemented

#The problem with doing it this way is that reading and analysing the EDF file takes a long time,
#   and dies if there is a problem with an EDF file, so it's probably better to do it incrementally
#   with a for loop!
#datWithEyeMetrics<- datWithEyeTracking |> rowwise() |>
#  reframe( EDFsummarise(pick(EDF_name),widthPix,heightPix,centralZoneWidthPix,centralZoneHeightPix,
#                        initialDurToExclude,driftCorrect,intervalToAssumeGazeCentral)         ) 

#Rather than doing it the fancy way above, do it separately so can save the result of analyzing all the 
#eyetracking files separately.
#Run EDF summarise on each row of the list of EDF names, https://gist.github.com/alexholcombe/2ec141beec36ac7640fa296dd236203c
EDFsToAnalyze<-  datWithEyeTracking |> 
                      group_by(IDnum,sessionNum) |>
                      summarize(EDF_name = first(EDF_name), .groups = 'drop')
pathsOnly<- EDFsToAnalyze$EDF_name
#pathsOnly<-pathsOnly[1:3] #Shorten for test run

# Call the function with each element of the list as the first parameter and concatenate the results into a tibble, https://gist.github.com/alexholcombe/2ec141beec36ac7640fa296dd236203c
EDFresults <- map_dfr(pathsOnly, ~ EDFsummarise(.x, widthPix,heightPix,centralZoneWidthPix,centralZoneHeightPix,
                                                    initialDurToExclude,driftCorrect,intervalToAssumeGazeCentral)   )
#It returns a weirdly-structured list actually, because some things only one value per file, others different value each trial,
#   which is in the perTrialStuff
eachTrialEDFs<- EDFresults$perTrialStuff
#Now add in the things that were not calculated for each trial, but I do want them as columns
#Isolate them by getting rid of the perTrialStuff
EDFresults$perTrialStuff <- NULL
#Now can combine them
EDFresults<- cbind(eachTrialEDFs,EDFresults)
#Eyelink counts from 1 for trialnum, whereas Psychopy starts with zero, need to subtract one from every trial
#In future, should use message sent to eyetracker of trial number.
EDFresults$trial <- EDFresults$trial - 1

#Add in the other columns (IDnum, sessionNum) from EDFsToAnalyze
EDFresults<- EDFsToAnalyze |> right_join(EDFresults, by=join_by(EDF_name) )
#table(EDFresults$outOfCentralArea,useNA="ifany")

#EDFresults |> filter(trial<3) #No NAs

#Now I have the following columns thanks to EDFsummarise:
# IDnum sessionNum  trial longestDist outOfCentralArea totalBlinkDur blinksTooLong pSamplesFailed anyDataLost

#Join these EDF results with the behavioral data, so can then start excluding trials.
datWithEyeMetrics<- datWithEyeTracking |> 
                      left_join(EDFresults, by=join_by(EDF_name,IDnum,sessionNum, trialnum==trial))
#table(datWithEyeMetrics$outOfCentralArea,useNA="ifany")
#datWithEyeMetrics |> filter(trialnum<3) |> select(trialnum,IDnum,sessionNum,timingBlips,EDFmatchExists,outOfCentralArea,longestDist) 

#RUN WITH TIMINGBLIPS ONLY EXCLUDED AND WITH BOTH TIMINGBLIPS AND EYEMOVMENET EXCLUSIONS

#Calculate number of trials per participant with timingBlips and with outOfCentralArea, save as .tsv for Momo
#Report on timingBlips
blipsSummary<- datWithEyeMetrics |> 
                  group_by(IDnum) |> 
                  summarise(pTrialsTooManyBlips =  mean(timingBlips > maxTimingBlipsToIncludeTrial) )
write_tsv(blipsSummary, file.path(dataDir,expName,"blipsSummary.tsv"))

#Report on eye metrics per participant and session
#Work out how much data is missing - because eyetracker didn't work? But I already filtered by 
eyeTrackingSummary<- datWithEyeMetrics |> 
    group_by(IDnum,sessionNum) |> 
    summarise(pNAs = mean( is.na( outOfCentralArea)  ), #How often could outOfCentralArea not be calculated? Dunno if no fixations can cause that
              pTrialsInCentralArea = 1 - mean(outOfCentralArea>0, na.rm=T),
              goodTrials = sum(outOfCentralArea==0))
write_tsv(eyeTrackingSummary, file.path(dataDir,expName,"eyeTrackingSummary.tsv"))

#Exclude timingBlips, maybe also trials where moved eyes
datAnalyze <- datWithEyeMetrics |> 
                filter(timingBlips <= maxTimingBlipsToIncludeTrial) |>
                filter(outOfCentralArea == 0)

#Report on how many trials per participant left after exclusions
numGoodTrials<- datAnalyze |> 
  group_by(IDnum) |> 
  summarise(goodTrials = sum(outOfCentralArea==0))
write_tsv(numGoodTrials, file.path(dataDir,expName,"numGoodTrialsPerParticipant.tsv"))

#Don't even try to curve fit if number of good trials is less than 30
participantsWithTooFewTrials <- numGoodTrials |> filter(goodTrials < 30) |> select(IDnum)

datAnalyze<- datAnalyze |> filter( !(IDnum %in% participantsWithTooFewTrials$IDnum ) )
  
#https://github.com/alexholcombe/speed-tf-VSS14/blob/master/analyseExps/doAllAnalyses_E4ab.R
iv<-"speed"
for (iv in c("speed","tf")) { #"logTf","logSpd"
  cat('Fitting data, extracting threshes, plotting with iv=',iv)
  
  source('analyzeMakeReadyForPlot.R') #returns fitParms for each subject, psychometrics, and function calcPctCorrThisSpeed
  #It crapped out on subject 30
  
  fitParms$iv<- iv
  #Get the plotIndividDataAndCurves function from executing the below file
  source('individDataWithPsychometricCurves.R') 
  
  factorsForPlot <- tibble( colorF = "offset", colF = "objects", rowF = "subject" )
  
  #Make a few plots of psychometric functions prior to extracting threshes
  datForThisPlot <- datAnalyze |> filter(  as.numeric(as.character(subject)) >= 27 ) |>
        filter(  as.numeric(as.character(subject)) <= 999 )
  psychometricsForThisPlot <- psychometrics |> filter( as.numeric(as.character(subject)) >=27  ) |>
    filter(  as.numeric(as.character(subject)) <= 999 )
  
  #Plot psychometric functions
  plt<- plotIndividDataAndCurves(expName,datForThisPlot,psychometricsForThisPlot,
                           factorsForPlot,wrapOrGrid=T,xmin=0,xmax=1.5) 
  plt<-plt+facet_wrap(vars(subject))
  show(plt)
  ggsave( file.path('figs', paste0('individPlotsE',expName,'.png'))  )
  
  #Psychometric doesn't go high enough with subject= 69  objects= 8  targets= 2, which
  #from the below individual graph looks appropriate - the persom was getting 25% wrong even at low speeds
  #sub69<- datAnalyze |> filter( subject==69)
  #s69<- plotIndividDataAndCurves("subject69",sub69,
  #                         psychometricsForThisPlot |> filter(subject==69),
  #                         tibble( colorF = "targets", colF = "targets", rowF = "objects" ),
  #                         wrapOrGrid=T,xmin=0,xmax=1.5)
  #show(s69)
  
  thrAll<-tibble()
  source("extractMomoThreshesAndPlot.R") #provides threshes
  #Add threshes to the plots so that can see where threshold extraction failed
  write_tsv(threshes,file.path("results","threshesMomo.tsv"))
  #below is old way, saving separate threshes
  #   varName=paste("threshes_",iv,"_",expName,sep='') #combine threshes
  #   assign(varName,threshes)
  #   save(list=varName,file=paste(dataDir,varName,".Rdata",sep='')) #e.g. threshes_tf_123targets269objects.Rdata
  #   cat("Saved",varName)
}
