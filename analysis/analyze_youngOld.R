#This file analyses anonymized data provided by "loadAnonymiseSaveData.R" in exp-specific directory
#This script expects the working directory to be "analysis"
rm(list = ls()) #Clear workspace
library(tidyverse)

dataDir<- file.path("..","dataAnonymized")
expName<- "youngOld"
dataDir<- file.path(dataDir,expName)

###############################################################################
### Read in the data files  ###################################################
#Read all the Psychopy data in, from the .tsv file generated by loadAnonymiseSaveData.R in dataPreprocess directory
anonDataFile<- paste0(expName,"_psychopy_data.tsv") 
anonDataFileWithPath<-file.path(dataDir,anonDataFile)
if (!file.exists(anonDataFileWithPath)) {
  message("Your anonymised datafile doesn't exist.")
}
rawData<- readr::read_tsv(anonDataFileWithPath, show_col_types=FALSE)

numSs<- length(unique(rawData$IDnum))
message("Have read in the data for ",numSs," anonymized participants.")

############
#Read the files info guide that provides mapping to EDF files, from the .tsv file generated by loadAnonymiseSaveData.R in dataPreprocess directory
filesGuidePath<- file.path(dataDir, paste0(expName,"_files_guide.tsv" ))
filesGuide<- readr::read_tsv(filesGuidePath, show_col_types=FALSE)

####################
#Get age and sex info - an anonymised copy of the Sharepoint participant sheet with many columns 
#  redacted that was created by loadAnonymiseSaveData.R
pInfo_path<- file.path(dataDir,"participantInfo_reducedForPrivacy.tsv")
pInfoReducedForPrivacy<- readr::read_tsv(pInfo_path, show_col_types=FALSE)
#Classify age
#ggplot(pInfoReducedForPrivacy,aes(x=agePerturbed)) +geom_histogram()
pInfoReducedForPrivacy<- pInfoReducedForPrivacy |> 
            mutate(age = if_else(agePerturbed>60, "Old", "Young"))

###Join age and sex to behavioural data
ageGender<- pInfoReducedForPrivacy |> select(IDnum,gender,age,agePerturbed)
#ALSO ADD ACUITY AND CROWDING for exploratory analyses
rawData<- rawData |> left_join(ageGender, by = join_by(IDnum))

###################################################
############Exclusion criteria########################
#################################################
excludeDidntReach75pct = TRUE
maxTimingBlipsToIncludeTrial = 6 #not implemented
#########################################
#Exclusions: eye-tracking related########
#See prereg update: We will NOT throw people out based on eyetracking data for the main analyses, instead looking at the effect of eye movement / not being able to track their eyes only as exploratory analyses.

excludeFixationViolations = FALSE; proportnTrialsMustFixate = .6;

initialDurToExclude<- 2000 #because trialMinTimeBeforeCuesOff=2
doDriftCorrect = FALSE;
intervalAssumeGazeCentral <- c(300,800); maxDistToDriftCorrect<-30 #See analyseIndividualPerson.qmd for rationale based on exploring data
##if excluding eye movements, eye-movement exclusion zone numbers
exclusionPixels <- 60 #Exploratory. See updated preregistration, and youngOld_eyeMovementExclusionCriteriaCheck 
centralZoneWidthPix = exclusionPixels*2
centralZoneHeightPix = exclusionPixels*2 #assumes the monitor is correct aspect ratio so that pixels are square

#Screen and viewing parameters
widthPix = 800
heightPix = 600
monitorWidth = 38 #cm
viewdist = 57 #cm
widthScreenDeg =  2*(atan((monitorWidth/2)/viewdist) /pi*180)
pixelsPerDegree = widthPix / widthScreenDeg

#Check counterbalancing to double-check nothing has gone wrong
source( file.path("helpers","checkCounterbalancing.R") )
t<- checkCombosOccurEqually(rawData, c("numObjects","numTargets"), tolerance=.01, verbose=F )
#Visually inspect trials for each combination for each subject
#table(rawData$numObjects,rawData$numTargets,rawData$IDnum)

# checkCombosOccurEqually(rawData, c("numObjects","numTargets","ringToQuery") )
# checkCombosOccurEqually(rawData, c("condition","leftOrRight") )
# checkCombosOccurEqually(rawData, c("condition","leftOrRight","offsetXYring0") ) #NO?
# checkCombosOccurEqually(rawData, c("numObjects","numTargets","speed") )  

datAnalyze <-rawData

datAnalyze$correct <- (datAnalyze$orderCorrect==3)
datAnalyze$correct<- as.numeric(datAnalyze$correct)
datAnalyze$chance <- 1 / datAnalyze$numObjects

#Make simpler names of variables for graphs for nicer plot labels, and make them factors
# for better plotting.
datAnalyze<- datAnalyze %>% dplyr::rename(objects = numObjects) %>% mutate(objects = as.factor(objects))
datAnalyze<- datAnalyze %>% dplyr::rename(targets = numTargets) %>% mutate(targets = as.factor(targets))

datAnalyze$subject<- datAnalyze$IDnum
datAnalyze$tf <- datAnalyze$speed * as.numeric( as.character(datAnalyze$objects) )
factorsForBreakdownForAnalysis <- c('objects','targets') #young

#If staircases work, average correct should be 0.794 in each condition
avgCorrOverall<- datAnalyze |> group_by(subject) |> summarise(mean=mean(correct),n=n())
avgCorr<- ggplot(avgCorrOverall,aes(x=subject,y=mean)) + geom_point()
#show(avgCorr)
#subject 69 is lowest with 64% correct, not terrible, but might be affected by excludeDidntReach75pct

######################################################################################################
##########################################
#EYE MOVEMENT analysis, currently non-functional as just copy-pasted from Momo, got this much working for her
#Potentially exclude trials where participants' eyes looked too far from center
#####
datWithEyeTracking<- datAnalyze

if (excludeFixationViolations) {
  #Delete P01 because something is screwy with subject ID 01, who somehow has 4 session 2's acccording to my derived records, whereas all others don’t have such duplicates
  #       Momo: sorry i totally forgot but P01 I had a problem with running session 2 where I ran it for 1 trials worth on the first run so ran another session with 3 trials worth to make it up to normal (4 trials per session). Also this was when there was a problem with running sessions titled ‘2’ so i believe the 1 trial session would have been P013 and the 3 trial session would have been P014.
  datWithEyeTracking<- datWithEyeTracking |> filter(datAnalyze$IDnum != "01")
  
  #Delete session (which might have letters instead of numbers) because redundant with sessionNum 
  datWithEyeTracking$session<-NULL; EDFmatchTable$session<- NULL
  
  #Join with EDFmatchTable so know the EDF file for each participant
  datWithEyeTracking<- datWithEyeTracking |> 
    left_join(EDFmatchTable, 
              by=join_by(IDnum,sessionNum, #colnames in next lines are redundant ones to avoid doubling them
                         comment,pTrialsLotsTimingBlips,pTrialsBlipsAfterFixatn,
                         pTrialsLongFramesAfterCue,EDFmatchExists))
  
  #Manually set some bad files' EDFmatchExists to FALSE so don't get error analysing them
  #j182.EDF is a corrupt EDF file, and Eyelink suggested diagnostics that Momo hasn't done
  datWithEyeTracking<- datWithEyeTracking %>% 
    mutate(EDFmatchExists = ifelse(str_starts(EDF_name,"j182"),
                                   FALSE, EDFmatchExists) )
  #j022 and p014 "won’t have any data recorded in the file from the problem i was having at the beginning of the experiment"-Momo
  datWithEyeTracking<- datWithEyeTracking %>% 
    mutate(EDFmatchExists = ifelse(str_starts(EDF_name,"j022"),
                                   FALSE, EDFmatchExists) )
  datWithEyeTracking<- datWithEyeTracking %>% 
    mutate(EDFmatchExists = ifelse(str_starts(EDF_name,"p014"),
                                   FALSE, EDFmatchExists) )
  
  #For each participant, analyze their EDF file and exclude data on that basis
  #But first need to handle those where EDFmatchExists==FALSE. Throw them out.
  datWithEyeTracking <- datWithEyeTracking |> filter(EDFmatchExists == TRUE)
  
  datWithEyeTracking$EDF_name<- str_c(datWithEyeTracking$EDF_name,".EDF")
  #Add path for EDF file
  datWithEyeTracking$EDF_name<- file.path(dataDir,expName,"AllEdfData",datWithEyeTracking$EDF_name)
  
  #Load summariseFromEDF.R to get its function
  # then for each EDF file, we decide whether to exclude each trial
  # After all that, join with datAnalyze
  source( file.path('..','dataPreprocess','eyetracking','summariseFromEDF.R') ) #defines EDFsummarise function
  
  driftCorrect<- FALSE #Currently not implemented
  intervalToAssumeGazeCentral<-c(0,0) #Currently not implemented
  
  #The problem with doing it this way is that reading and analysing the EDF file takes a long time,
  #   and dies if there is a problem with an EDF file, so it's probably better to do it incrementally
  #   with a for loop!
  #datWithEyeMetrics<- datWithEyeTracking |> rowwise() |>
  #  reframe( EDFsummarise(pick(EDF_name),widthPix,heightPix,centralZoneWidthPix,centralZoneHeightPix,
  #                        initialDurToExclude,driftCorrect,intervalToAssumeGazeCentral)         ) 
  
  #Rather than doing it the fancy way above, do it separately so can save the result of analyzing all the 
  #eyetracking files separately.
  #Run EDF summarise on each row of the list of EDF names, https://gist.github.com/alexholcombe/2ec141beec36ac7640fa296dd236203c
  EDFsToAnalyze<-  datWithEyeTracking |> 
    group_by(IDnum,sessionNum) |>
    summarize(EDF_name = first(EDF_name), .groups = 'drop')
  pathsOnly<- EDFsToAnalyze$EDF_name
  #pathsOnly<-pathsOnly[1:3] #Shorten for test run
  
  # Call the function with each element of the list as the first parameter and concatenate the results into a tibble, https://gist.github.com/alexholcombe/2ec141beec36ac7640fa296dd236203c
  EDFresults <- map_dfr(pathsOnly, ~ EDFsummarise(.x, widthPix,heightPix,centralZoneWidthPix,centralZoneHeightPix,
                                                  initialDurToExclude,driftCorrect,intervalToAssumeGazeCentral)   )
  #It returns a weirdly-structured list actually, because some things only one value per file, others different value each trial,
  #   which is in the perTrialStuff
  eachTrialEDFs<- EDFresults$perTrialStuff
  #Now add in the things that were not calculated for each trial, but I do want them as columns
  #Isolate them by getting rid of the perTrialStuff
  EDFresults$perTrialStuff <- NULL
  #Now can combine them
  EDFresults<- cbind(eachTrialEDFs,EDFresults)
  #Eyelink counts from 1 for trialnum, whereas Psychopy starts with zero, need to subtract one from every trial
  #In future, should use message sent to eyetracker of trial number.
  EDFresults$trial <- EDFresults$trial - 1
  
  #Add in the other columns (IDnum, sessionNum) from EDFsToAnalyze
  EDFresults<- EDFsToAnalyze |> right_join(EDFresults, by=join_by(EDF_name) )
  #table(EDFresults$outOfCentralArea,useNA="ifany")
  
  #EDFresults |> filter(trial<3) #No NAs
  
  #Now I have the following columns thanks to EDFsummarise:
  # IDnum sessionNum  trial longestDist outOfCentralArea totalBlinkDur blinksTooLong pSamplesFailed anyDataLost
  
  #Join these EDF results with the behavioral data, so can then start excluding trials.
  datWithEyeMetrics<- datWithEyeTracking |> 
    left_join(EDFresults, by=join_by(EDF_name,IDnum,sessionNum, trialnum==trial))
  #table(datWithEyeMetrics$outOfCentralArea,useNA="ifany")
  #datWithEyeMetrics |> filter(trialnum<3) |> select(trialnum,IDnum,sessionNum,timingBlips,EDFmatchExists,outOfCentralArea,longestDist) 
  
  #Report on eye metrics per participant and session
  #Work out how much data is missing - because eyetracker didn't work? But I already filtered by 
  eyeTrackingSummary<- datWithEyeMetrics |> 
    group_by(IDnum,sessionNum) |> 
    summarise(pNAs = mean( is.na( outOfCentralArea)  ), #How often could outOfCentralArea not be calculated? Dunno if no fixations can cause that
              pTrialsInCentralArea = 1 - mean(outOfCentralArea>0, na.rm=T),
              goodTrials = sum(outOfCentralArea==0))
  write_tsv(eyeTrackingSummary, file.path(dataDir,expName,"eyeTrackingSummary.tsv"))
} #end if excludeFixationViolations
############################################################################################################
#### TIMING BLIPS ###############################################################
#########################################
#Exclude timingBlips. Preregistration update says no trials will be excluded (only trials with more than 3 frame misses after the cues turn off will be excluded (which is no trials, although it's more complicated than that because some sessions that wasn't logged)

#Calculate number of trials per participant with timingBlips and with outOfCentralArea, save as .tsv for Momo
#Report on timingBlips
# blipsSummary<- datWithEyeMetrics |> 
#   group_by(IDnum) |> 
#   summarise(pTrialsTooManyBlips =  mean(timingBlips > maxTimingBlipsToIncludeTrial) )
# write_tsv(blipsSummary, file.path(dataDir,expName,"blipsSummary.tsv"))
# blipsFirstRecordsEachParticipant<- datWithEyeTracking |> 
#   group_by(IDnum) |> summarise(timingBlips = first(timingBlips),
#                                numLongFramesAfterFixation = first(numLongFramesAfterFixation),
#                                numLongFramesAfterCue = first(numLongFramesAfterCue))
# 
# blipsExcessiveEachParticipant<- datWithEyeTracking |> 
#   group_by(IDnum,session) |> summarise(nExcessiveTimingBlips = sum(timingBlips>maxTimingBlipsToIncludeTrial),
#                                        nExcessiveLongFramesAfterFixation = sum(numLongFramesAfterFixation>maxTimingBlipsToIncludeTrial),
#                                        nExcessiveLongFramesAfterCue = sum(numLongFramesAfterCue>maxTimingBlipsToIncludeTrial))
# 
# blipsSummary<- datWithEyeTracking |> 
#   group_by(IDnum) |> 
#   summarise(pTrialsManyBlips =  mean(timingBlips > maxTimingBlipsToIncludeTrial) )
# 
# blipsSummary<- datWithEyeMetrics |> 
#   group_by(IDnum) |> 
#   summarise(pTrialsLotsTimingBlips =  mean(pTrialsLotsTimingBlips),
#             pTrialsBlipsAfterFixatn = mean(pTrialsBlipsAfterFixatn),
#             pTrialsLongFramesAfterCue = mean(pTrialsLongFramesAfterCue))

############################################################################################################
#### Fit data and extract thresholds ###############################################################
#########################################
iv<-"speed" #tf
for (iv in c("speed","tf")) { #"logTf","logSpd"
  cat('Fitting data, extracting threshes, plotting with iv=',iv)
  
  #The following returns fitParms for each subject, psychometrics, and function calcPctCorrThisSpeed
  source('analyzeMakeReadyForPlot.R')
  #Join with age gender info
  fitParms$subject<- as.numeric( as.character(fitParms$subject) )
  fitParms<-fitParms |> left_join(ageGender, by = join_by(subject==IDnum))
  fitParms$iv<- iv
  
  #Plot some psychometric functions; look out for data that doesn't go low or high enough to 
  # get the threshold.
  #Get the plotIndividDataAndCurves function
  source('individDataWithPsychometricCurves.R') 
  factorsForPlot <- tibble( colorF = "targets", colF = "objects", rowF = "subject" )
  
  #Make a few plots of psychometric functions
  maxSubjForPlot<-37
  datForThisPlot <- datAnalyze |> filter(  as.numeric(as.character(subject)) <= maxSubjForPlot )
  psychometricsForThisPlot <- psychometrics |> 
                                  filter( as.numeric(as.character(subject)) <=maxSubjForPlot  )
  xmax=1.5
  if (iv=="tf") { xmax=6 }
  plt<- plotIndividDataAndCurves(expName,datForThisPlot,psychometricsForThisPlot,
                           iv,factorsForPlot,wrapOrGrid=T,xmin=0,xmax=xmax) 
  plt<-plt+facet_wrap(vars(subject))
  show(plt)
  ggsave( file.path('figs', paste0('individPlotsE',expName,'.png'))  )
  
  #Even at slowest speeds, subject= 69  objects= 8  targets= 2 barely gets up to 75% correct.
  #See below individual graph looks appropriate
  sub69<- datAnalyze |> filter( subject==69)
  s69<- plotIndividDataAndCurves("subject69",sub69,
                           psychometrics |> filter(subject==69), iv,
                           tibble( colorF = "targets", colF = "targets", rowF = "objects" ),
                           wrapOrGrid=T,xmin=0,xmax=1.5)
  
  show(s69)
  #Plot another special subject individually
  specialSubject<-54 #54 Doesn't go up to three-quarters thresh for 8 objects (78%)
  specialSubjectData<- datAnalyze |> filter( subject==specialSubject)
  specialSubjectPlot<- plotIndividDataAndCurves(as.character(specialSubject),specialSubjectData,
                                 psychometrics |> filter(subject==specialSubject), iv,
                                 tibble( colorF = "targets", colF = "targets", rowF = "objects" ),
                                 wrapOrGrid=T,xmin=0,xmax=1.5)
  show(specialSubjectPlot)  
  
  thrAll<-tibble()
  source("extractThreshesAndPlot.R") #provides threshes
  
  #Psychometric functions, create publishable plots
  #Add threshes to the plots so that can see where threshold extraction failed
  thrAll<-rbind(thrAll,threshes)
  #below is old way, saving separate threshes
  #   varName=paste("threshes_",iv,"_",expName,sep='') #combine threshes
  #   assign(varName,threshes)
  #   save(list=varName,file=paste(dataDir,varName,".Rdata",sep='')) #e.g. threshes_tf_123targets269objects.Rdata
  #   cat("Saved",varName)
}
